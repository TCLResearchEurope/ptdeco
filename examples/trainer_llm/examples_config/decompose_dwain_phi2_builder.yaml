ptdeco_trainer_llm_version: "0.1.18"
ptdeco_version: "0.5.7"

task: "decompose_dwain"

# Model specification

decomposed_model_name: "microsoft/phi-2"
decomposed_model_revision: "main"
decomposed_model_custom_builder_path: "examples_builder/bp_builder.py"
decomposed_model_custom_builder_config:
  bp_model_path: "/nas/people/michal_sokolski/phi-small-prune/bp_ckpt_step_TEST/"
  bp_load_state_dict: true
decomposed_model_enable_gradient_checkpointing: false
decomposed_model_dtype: "torch.bfloat16"


# Data handling params

decomposition_data_name: "alpaca.full"
decomposition_data_separator: "\n\n" # Normal pythonic string, no extra escaping needed
decomposition_data_max_length: 2048
decomposition_data_batch_size: 1

perplexity_data_name: "wikitext2.test"
perplexity_data_separator: "\n\n"  # Normal pythonic string, no extra escaping needed
perplexity_data_max_length: 2048
perplexity_data_batch_size: 1

# Decomposition params

num_data_steps: 2048
num_metric_steps: 32
trade_off_factor: 3.0
reduction_factor: 0.5
max_accepted_ppl_diff: 0.1
nsr_final_threshold: 1.0
min_rank: 4
decompose_in_float64: true
precomputing_covariance_num_splits: 4


# Fintetuning params

finetuning_run: true
finetuning_use_lora: true
finetuning_lora_min_rank: 32
finetuning_lr: 0.0001
finetuning_num_steps: 50
finetuning_num_last_finetuned_modules: 8
finetuning_use_rank_pattern: false

# lm_eval evaluation params
lm_eval_initial: false
lm_eval_tasks:
  - arc_challenge
  - arc_easy
  - piqa
  - hellaswag
  - winogrande
  - ceval-valid
  - cmmlu

blacklisted_modules:
  - lm_head
